{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import math\n",
    "#import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GazeEstimationDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, trans=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.trans = trans\n",
    "        self.frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.frame.iloc[idx, 0]\n",
    "        img = cv2.imread(img_name)\n",
    "        img_norm = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        #rgb_img = np.repeat(image[..., np.newaxis], 3, -1)\n",
    "        face_img_coor = np.fromstring(self.frame.iloc[idx, 3][1:int(len(self.frame.iloc[idx, 3]) - 1)],\n",
    "                                      sep=',', dtype=int)\n",
    "        face_img = img_norm[face_img_coor[0]: face_img_coor[0] + face_img_coor[2],\n",
    "                   face_img_coor[1]: face_img_coor[1] + face_img_coor[3], :]\n",
    "\n",
    "        opt_flow_face = np.fromstring(self.frame.iloc[idx, 5][0][1:int(len(self.frame.iloc[idx, 4]) - 1)],\n",
    "                                      sep=',', dtype=int)\n",
    "        sample = {'face': face_img, 'opt_flow': opt_flow_face, 'x': self.frame.iloc[idx, 1], 'y': self.frame.iloc[idx, 2]}\n",
    "        if self.trans:\n",
    "            sample = self.trans(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['face']\n",
    "\n",
    "        # h, w = image.shape[:2]\n",
    "        # if isinstance(self.output_size, int):\n",
    "        #    if h > w:\n",
    "        #        new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        #    else:\n",
    "        #        new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        # else:\n",
    "        new_h, new_w = self.output_size, self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(img, (new_h, new_w))\n",
    "\n",
    "        x, y = sample['x'], sample['y']\n",
    "        opt_flow = sample['opt_flow']\n",
    "\n",
    "        return {'face': img, 'opt_flow': opt_flow, 'x': x, 'y': y}\n",
    "\n",
    "    '''opt_flow': opt_flow'''\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        img = sample['face']\n",
    "        opt_flow = sample['opt_flow']\n",
    "        x, y = (sample['x'] + 800) / 1600, (sample['y'] + 800) / 1600\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        return {'face': torch.from_numpy(img).type(torch.DoubleTensor),\n",
    "                'opt_flow': torch.from_numpy(opt_flow).type(torch.DoubleTensor),\n",
    "                'gt_coor': torch.tensor([x, y]).type(torch.DoubleTensor)}\n",
    "\n",
    "    '''opt_flow': torch.from_numpy(opt_flow.values)'''\n",
    "\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 15, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(15, 5, 8)\n",
    "        self.pool2 = nn.MaxPool2d(3,3)\n",
    "        self.conv3 = nn.Conv2d(5, 3, 16)\n",
    "        self.fc1 = nn.Linear(59536, 120)\n",
    "        self.d1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.d2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "class NetFace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(59536, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class GazeEstimationDatasetEyes(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, trans=None):\n",
    "        self.frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.trans = trans\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.frame))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.frame.iloc[idx, 0]\n",
    "        opt_flow_eyes = np.fromstring(self.frame.iloc[idx, 5][1][1:int(len(self.frame.iloc[idx, 4]) - 1)],\n",
    "                                      sep=',', dtype=int)\n",
    "\n",
    "        img = cv2.imread(img_name)\n",
    "        img_norm = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        eyes_roi = np.fromstring(self.frame.iloc[idx, 4][1:int(len(self.frame.iloc[idx, 4]) - 1)],\n",
    "                                      sep=',', dtype=int)\n",
    "        x_eye, y_eye, w, h = eyes_roi[0], eyes_roi[1], eyes_roi[2], eyes_roi[3]\n",
    "\n",
    "        eyes_img = img_norm[y_eye: y_eye + h, x_eye: x_eye + w]\n",
    "\n",
    "        x_gt = (self.frame.iloc[idx, 1] + 800) / 1600\n",
    "        y_gt = (self.frame.iloc[idx, 2] + 800) / 1600\n",
    "\n",
    "        sample = {'eyes_img': eyes_img, 'opt_flow': opt_flow_eyes, 'x': x_gt, 'y': y_gt}\n",
    "\n",
    "        if self.trans:\n",
    "            sample = self.trans(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class RescaleEyes(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['eyes_img']\n",
    "\n",
    "        # h, w = image.shape[:2]\n",
    "        # if isinstance(self.output_size, int):\n",
    "        #    if h > w:\n",
    "        #        new_h, new_w = self.output_size * h / w, self.output_size\n",
    "        #    else:\n",
    "        #        new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        # else:\n",
    "        new_h, new_w = self.output_size, self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(img, (new_h, new_w))\n",
    "\n",
    "        x, y = sample['x'], sample['y']\n",
    "        opt_flow = sample['opt_flow']\n",
    "\n",
    "        return {'eyes_img': img, 'opt_flow': opt_flow, 'x': x, 'y': y}\n",
    "\n",
    "class ToTensorEyes(object):\n",
    "    def __call__(self, sample):\n",
    "        img = sample['eyes_img']\n",
    "        x, y = sample['x'], sample['y']\n",
    "        opt_flow = sample['opt_flow']\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        return {'eyes_img': torch.from_numpy(img).type(torch.DoubleTensor),\n",
    "                'opt_flow': torch.from_numpy(opt_flow).type(torch.DoubleTensor),\n",
    "                'gt_coor': torch.tensor([x, y]).type(torch.DoubleTensor)}\n",
    "\n",
    "class NetEyes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(13456, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.pool(F.relu(self.conv1(x1)))\n",
    "        x1 = self.pool(F.relu(self.conv2(x1)))\n",
    "        x1 = torch.flatten(x1, 1)  # flatten all dimensions except batch\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        x1 = self.fc3(x1)\n",
    "\n",
    "        return x1, x2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = GazeEstimationDatasetEyes(csv_file=\"full_face/total_opt_flow.csv\", root_dir=\"\")\n",
    "transformed_dataset = GazeEstimationDataset(csv_file=\"full_face/total_opt_flow.csv\", root_dir=\"\",\n",
    "                                            trans=transforms.Compose([Rescale(256), ToTensor()]))\n",
    "transformed_dataset_eyes = GazeEstimationDatasetEyes(csv_file=\"full_face/total_opt_flow.csv\", root_dir=\"\",\n",
    "                                            trans=transforms.Compose([RescaleEyes(128), ToTensorEyes()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "datasets containing full face pictures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(transformed_dataset))\n",
    "test_size = len(transformed_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(transformed_dataset, [train_size, test_size])\n",
    "\n",
    "train_size = int(0.95 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "net = NetFace().to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 0: 100%|██████████| 212/212 [09:13<00:00,  2.61s/batch, distance=14.8, train_loss=0.0128] \n",
      "Validation 0: 100%|██████████| 12/12 [00:18<00:00,  1.57s/batch, val_acc=15.2, val_loss=0.0361]\n",
      "Training 1: 100%|██████████| 212/212 [08:15<00:00,  2.34s/batch, distance=7.01, train_loss=0.00278]\n",
      "Validation 1: 100%|██████████| 12/12 [00:19<00:00,  1.60s/batch, val_acc=12.3, val_loss=0.0315]\n",
      "Training 2: 100%|██████████| 212/212 [08:15<00:00,  2.34s/batch, distance=9.33, train_loss=0.0063] \n",
      "Validation 2: 100%|██████████| 12/12 [00:23<00:00,  1.92s/batch, val_acc=5.72, val_loss=0.00511]\n",
      "Training 3: 100%|██████████| 212/212 [08:19<00:00,  2.36s/batch, distance=6.38, train_loss=0.00232] \n",
      "Validation 3: 100%|██████████| 12/12 [00:19<00:00,  1.58s/batch, val_acc=7.55, val_loss=0.00929]\n",
      "Training 4: 100%|██████████| 212/212 [08:17<00:00,  2.34s/batch, distance=6.81, train_loss=0.00279]\n",
      "Validation 4: 100%|██████████| 12/12 [00:20<00:00,  1.74s/batch, val_acc=6.73, val_loss=0.00725]\n",
      "Training 5: 100%|██████████| 212/212 [08:19<00:00,  2.36s/batch, distance=6.5, train_loss=0.0026]   \n",
      "Validation 5: 100%|██████████| 12/12 [00:18<00:00,  1.58s/batch, val_acc=11, val_loss=0.0194]  \n",
      "Training 6: 100%|██████████| 212/212 [08:20<00:00,  2.36s/batch, distance=5.16, train_loss=0.00174] \n",
      "Validation 6: 100%|██████████| 12/12 [00:23<00:00,  1.94s/batch, val_acc=8.66, val_loss=0.012] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "min_valid_loos = np.inf\n",
    "for epoch in range(n):\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0.0\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Training {epoch}\")\n",
    "            faces = data['face']\n",
    "            labels = data['gt_coor']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = net(faces)\n",
    "\n",
    "            train_loss = criterion(output, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_dist = 0\n",
    "\n",
    "            for i in range (len(output)):\n",
    "                out_x, out_y = output[i][0], output[i][1]\n",
    "                lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                dx = out_x - lab_x\n",
    "                dy = out_y - lab_y\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                batch_dist += dist\n",
    "\n",
    "\n",
    "            #correct = (abs(output - labels)).sum().item()\n",
    "            distance = batch_dist / len(output)\n",
    "            running_loss += train_loss.item()\n",
    "            tepoch.set_postfix(train_loss=train_loss.item(), distance=100*distance)\n",
    "\n",
    "        #print(\"train loss value: \", running_loss/len(trainloader))\n",
    "\n",
    "    with tqdm(valloader, unit=\"batch\") as tepoch:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Validation {epoch}\")\n",
    "                faces = data['face']\n",
    "                labels = data['gt_coor']\n",
    "                #labels = [0.5, 0.5]\n",
    "                #labels = torch.Tensor(labels).type(torch.DoubleTensor)\n",
    "                output = net(faces)\n",
    "                loss = criterion(output, labels)\n",
    "                val_loss = loss.item()*faces.size(0)\n",
    "                #dec = min_valid_loos > val_loss\n",
    "                #if dec:\n",
    "                #    min_valid_loos = val_loss\n",
    "                batch_dist = 0\n",
    "                for i in range (len(output)):\n",
    "                    out_x, out_y = output[i][0], output[i][1]\n",
    "                    lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                    dx = out_x - lab_x\n",
    "                    dy = out_y - lab_y\n",
    "                    dist = math.sqrt(dx*dx + dy*dy)\n",
    "                    batch_dist += dist\n",
    "\n",
    "                #correct = (abs(output - labels)).sum().item()\n",
    "                val_dist = batch_dist / len(output)\n",
    "                tepoch.set_postfix(val_loss=val_loss, val_acc=100*val_dist)\n",
    "\n",
    "        #print(\"validation done for current epoch\")\n",
    "\n",
    "\n",
    "print('done')\n",
    "path = './trained_face.pth'\n",
    "torch.save(net.state_dict(), path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:50<00:00,  1.98s/batch, distance=3.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "[0.05949966309459023, 0.05008026359080435, 0.049768659299329845, 0.04446968858562382, 0.06018011717998062, 0.06520673701226878, 0.054877263733902705, 0.05257904073971813, 0.0654016836113503, 0.04631345636011396, 0.037437151873058856, 0.05287306299666801, 0.06530292640592636, 0.05040494480239445, 0.0625805197268185, 0.041152478204878454, 0.05512800461471632, 0.05201234359598573, 0.054557489941675294, 0.056298208386637634, 0.052464191443160585, 0.05000741437636013, 0.06917310803758644, 0.0524028067617749, 0.06694999963731285, 0.04598571104888542, 0.05056822905929117, 0.06839889129801113, 0.0596348074286696, 0.050054309815522016, 0.048046268244693234, 0.048398270123249106, 0.04453860711864628, 0.04768868600521359, 0.04796741313275347, 0.08302831311896339, 0.05876924737572374, 0.0480315858407602, 0.04482895411587731, 0.06476792094868924, 0.04908412966978647, 0.049750267276777685, 0.052077996718446144, 0.04105272163071614, 0.04605363337644323, 0.06619465842824361, 0.06746140996674176, 0.06885218287628284, 0.050581782625406024, 0.052468702071874956, 0.04664468862570325, 0.047432427065132225, 0.05626103378308799, 0.06062978083249346, 0.05056598910532639, 0.033574420494156924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "path = './trained_face.pth'\n",
    "net.load_state_dict(torch.load(path))\n",
    "#print(net)\n",
    "distances = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(testloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            image = data['face']\n",
    "            #labels = [0.5, 0.5]\n",
    "            labels = data['gt_coor']\n",
    "            output = net(image)\n",
    "            dim = len(output)\n",
    "            batch_dist = 0\n",
    "            for i in range(dim):\n",
    "                out_x, out_y = output[i][0], output[i][1]\n",
    "                lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                dx = out_x - lab_x\n",
    "                dy = out_y - lab_y\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                batch_dist += dist\n",
    "\n",
    "            distance = batch_dist / len(output)\n",
    "            distances.append(distance)\n",
    "            tepoch.set_postfix(distance=100*distance)\n",
    "print(\"done\")\n",
    "print(distances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataset containing only eye portion 40FPS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(transformed_dataset_eyes))\n",
    "test_size = len(transformed_dataset_eyes) - train_size\n",
    "train_dataset_eyes, test_dataset_eyes = torch.utils.data.random_split(transformed_dataset_eyes, [train_size, test_size])\n",
    "\n",
    "train_size = int(0.95 * len(train_dataset_eyes))\n",
    "val_size = len(train_dataset_eyes) - train_size\n",
    "train_dataset_eyes, val_dataset_eyes = torch.utils.data.random_split(train_dataset_eyes, [train_size, val_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "trainloader_eyes = DataLoader(train_dataset_eyes, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valloader_eyes = DataLoader(val_dataset_eyes, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader_eyes = DataLoader(test_dataset_eyes, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "net = NetEyes().to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 0: 100%|██████████| 212/212 [03:13<00:00,  1.10batch/s, distance=27.9, train_loss=0.0491]\n",
      "Validation 0: 100%|██████████| 12/12 [00:08<00:00,  1.48batch/s, val_acc=33.1, val_loss=0.171]\n",
      "Training 1: 100%|██████████| 212/212 [02:57<00:00,  1.20batch/s, distance=17.2, train_loss=0.0172] \n",
      "Validation 1: 100%|██████████| 12/12 [00:07<00:00,  1.64batch/s, val_acc=16.6, val_loss=0.0697]\n",
      "Training 2: 100%|██████████| 212/212 [02:54<00:00,  1.21batch/s, distance=13.4, train_loss=0.0199] \n",
      "Validation 2: 100%|██████████| 12/12 [00:07<00:00,  1.64batch/s, val_acc=11.5, val_loss=0.0228]\n",
      "Training 3: 100%|██████████| 212/212 [02:53<00:00,  1.22batch/s, distance=14.5, train_loss=0.0139] \n",
      "Validation 3: 100%|██████████| 12/12 [00:07<00:00,  1.70batch/s, val_acc=6.84, val_loss=0.0101]\n",
      "Training 4: 100%|██████████| 212/212 [02:54<00:00,  1.22batch/s, distance=20.1, train_loss=0.0293] \n",
      "Validation 4: 100%|██████████| 12/12 [00:07<00:00,  1.63batch/s, val_acc=7.96, val_loss=0.0116]\n",
      "Training 5: 100%|██████████| 212/212 [02:49<00:00,  1.25batch/s, distance=12.7, train_loss=0.0139] \n",
      "Validation 5: 100%|██████████| 12/12 [00:07<00:00,  1.60batch/s, val_acc=8.12, val_loss=0.0128]\n",
      "Training 6: 100%|██████████| 212/212 [03:00<00:00,  1.18batch/s, distance=13.6, train_loss=0.0132] \n",
      "Validation 6: 100%|██████████| 12/12 [00:08<00:00,  1.45batch/s, val_acc=15.6, val_loss=0.0458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "min_valid_loos = np.inf\n",
    "for epoch in range(n):\n",
    "    with tqdm(trainloader_eyes, unit=\"batch\") as tepoch:\n",
    "        running_loss = 0.0\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Training {epoch}\")\n",
    "            inputs = data['eyes_img']\n",
    "            labels = data['gt_coor']\n",
    "            optical_flow = data['opt_flow']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = net(inputs)\n",
    "\n",
    "            train_loss = criterion(output, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_dist = 0\n",
    "\n",
    "            for i in range (len(output)):\n",
    "                out_x, out_y = output[i][0], output[i][1]\n",
    "                lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                dx = out_x - lab_x\n",
    "                dy = out_y - lab_y\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                batch_dist += dist\n",
    "\n",
    "\n",
    "            #correct = (abs(output - labels)).sum().item()\n",
    "            distance = batch_dist / len(output)\n",
    "            running_loss += train_loss.item()\n",
    "            tepoch.set_postfix(train_loss=train_loss.item(), distance=100*distance)\n",
    "\n",
    "        #print(\"train loss value: \", running_loss/len(trainloader))\n",
    "\n",
    "    with tqdm(valloader_eyes, unit=\"batch\") as tepoch:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Validation {epoch}\")\n",
    "                inputs = data['eyes_img']\n",
    "                labels = data['gt_coor']\n",
    "                #labels = [0.5, 0.5]\n",
    "                #labels = torch.Tensor(labels).type(torch.DoubleTensor)\n",
    "                output = net(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                val_loss = loss.item()*inputs.size(0)\n",
    "                #dec = min_valid_loos > val_loss\n",
    "                #if dec:\n",
    "                #    min_valid_loos = val_loss\n",
    "                batch_dist = 0\n",
    "                for i in range (len(output)):\n",
    "                    out_x, out_y = output[i][0], output[i][1]\n",
    "                    lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                    dx = out_x - lab_x\n",
    "                    dy = out_y - lab_y\n",
    "                    dist = math.sqrt(dx*dx + dy*dy)\n",
    "                    batch_dist += dist\n",
    "\n",
    "                #correct = (abs(output - labels)).sum().item()\n",
    "                val_dist = batch_dist / len(output)\n",
    "                tepoch.set_postfix(val_loss=val_loss, val_acc=100*val_dist)\n",
    "\n",
    "        #print(\"validation done for current epoch\")\n",
    "\n",
    "\n",
    "print('done')\n",
    "path = './trained_eyes.pth'\n",
    "torch.save(net.state_dict(), path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:03<00:00,  1.13s/batch, distance=11.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "[0.12739867741249106, 0.09682358642166573, 0.08164905252296582, 0.09630584700019643, 0.09587479534256883, 0.11953552710988823, 0.0636588508440594, 0.09676803806236373, 0.11630367745271228, 0.11247561559901084, 0.11312693093211175, 0.11385132228717781, 0.09680331447751142, 0.0957849750326049, 0.07190886332558279, 0.06707805867741085, 0.09860276822285488, 0.10981700551410885, 0.11797860999200772, 0.10199024381985942, 0.08362565417991831, 0.09790807099261754, 0.06609095476282197, 0.10382791859815178, 0.08434410285654992, 0.10175774664616283, 0.07944261670650951, 0.09306411229481222, 0.09180763790448282, 0.11642367890964543, 0.0844357677636685, 0.10486390031321458, 0.10970180833165312, 0.16795082698862393, 0.12877746632424933, 0.10772881507813083, 0.1005329992585542, 0.09109110041059862, 0.08288591814377742, 0.07724711224557818, 0.1399541185519757, 0.1074644732561188, 0.10683463557550178, 0.11445796315709313, 0.09572891366128175, 0.1485456672980298, 0.08762541677775527, 0.08844907455495092, 0.09828102596197502, 0.07622416597865939, 0.11692587583616335, 0.12504205848716243, 0.10413736297306563, 0.14479274584389443, 0.09283073664703262, 0.11373318828948316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "path = './trained_eyes.pth'\n",
    "net.load_state_dict(torch.load(path))\n",
    "#print(net)\n",
    "distances = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(testloader_eyes, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            image = data['eyes_img']\n",
    "            labels = data['gt_coor']\n",
    "            # labels = [0.5, 0.5]\n",
    "            output = net(image)\n",
    "            dim = len(output)\n",
    "            batch_dist = 0\n",
    "            for i in range(dim):\n",
    "                out_x, out_y = output[i][0], output[i][1]\n",
    "                lab_x, lab_y = labels[i][0], labels[i][1]\n",
    "                dx = out_x - lab_x\n",
    "                dy = out_y - lab_y\n",
    "                dist = math.sqrt(dx*dx + dy*dy)\n",
    "                batch_dist += dist\n",
    "\n",
    "            distance = batch_dist / len(output)\n",
    "            distances.append(distance)\n",
    "            tepoch.set_postfix(distance=100*distance)\n",
    "print(\"done\")\n",
    "print(distances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}